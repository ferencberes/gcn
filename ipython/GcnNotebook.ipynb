{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time, sys\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gcn.utils import *\n",
    "from gcn.models import GCN, MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_string('dataset', 'cora', 'Dataset string.')  # 'cora', 'citeseer', 'pubmed'\n",
    "flags.DEFINE_string('model', 'gcn', 'Model string.')  # 'gcn', 'gcn_cheby', 'dense'\n",
    "flags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate.')\n",
    "flags.DEFINE_integer('epochs', 200, 'Number of epochs to train.')\n",
    "flags.DEFINE_integer('hidden1', 16, 'Number of units in hidden layer 1.')\n",
    "flags.DEFINE_float('dropout', 0.5, 'Dropout rate (1 - keep probability).')\n",
    "flags.DEFINE_float('weight_decay', 5e-4, 'Weight for L2 loss on embedding matrix.')\n",
    "flags.DEFINE_integer('early_stopping', 10, 'Tolerance for early stopping (# of epochs).')\n",
    "flags.DEFINE_integer('max_degree', 3, 'Maximum Chebyshev polynomial degree.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lil_mx = sp.lil_matrix(np.array([[1,2],[3,4],[5,6]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1\n",
      "  (0, 1)\t2\n"
     ]
    }
   ],
   "source": [
    "print(lil_mx[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vstacked_lil = sp.vstack((np.array([[1,2,3],[4,5,6]]),np.array([1,2,3]))).tolil()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1\n",
      "  (0, 1)\t2\n",
      "  (0, 2)\t3\n"
     ]
    }
   ],
   "source": [
    "print(vstacked_lil[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.16666667,  0.06666667])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.array([[1,2,3],[4,5,6]])\n",
    "rowsum = arr.sum(axis=1)\n",
    "np.power(rowsum,-1.0).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data(FLAGS.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some preprocessing (run only once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = preprocess_features(features)\n",
    "if FLAGS.model == 'gcn':\n",
    "    support = [preprocess_adj(adj)]\n",
    "    num_supports = 1\n",
    "    model_func = GCN\n",
    "elif FLAGS.model == 'gcn_cheby':\n",
    "    support = chebyshev_polynomials(adj, FLAGS.max_degree)\n",
    "    num_supports = 1 + FLAGS.max_degree\n",
    "    model_func = GCN\n",
    "elif FLAGS.model == 'dense':\n",
    "    support = [preprocess_adj(adj)]  # Not used\n",
    "    num_supports = 1\n",
    "    model_func = MLP\n",
    "else:\n",
    "    raise ValueError('Invalid argument for model: ' + str(FLAGS.model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at preprocessed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjacency matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "adj.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature matrix: Stored as a sparse matrix in a dict\n",
    "   * coordinates\n",
    "   * values for coordinates\n",
    "   * shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49216, 2)\n",
      "(49216,)\n",
      "(2708, 1433)\n"
     ]
    }
   ],
   "source": [
    "print(features[0].shape)\n",
    "print(features[1].shape)\n",
    "print(features[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels: are onehot encoded\n",
    "\n",
    "The number of columns is the number of different groups in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2708, 7)\n",
      "(2708, 7)\n",
      "(2708, 7)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the training data only 20 entity is revealed from each group (for [Cora](https://relational.fit.cvut.cz/dataset/CORA) citiation network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 20.  20.  20.  20.  20.  20.  20.]\n",
      "[ 130.   91.  144.  319.  149.  103.   64.]\n",
      "[  61.   36.   78.  158.   81.   57.   29.]\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(y_train,axis=0))\n",
    "print(np.sum(y_test,axis=0))\n",
    "print(np.sum(y_val,axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masks\n",
    "boolean vectors which indicate where are the train, validation and test records in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2708,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "placeholders = {\n",
    "    'support': [tf.sparse_placeholder(tf.float32) for _ in range(num_supports)],\n",
    "    'features': tf.sparse_placeholder(tf.float32, shape=tf.constant(features[2], dtype=tf.int64)),\n",
    "    'labels': tf.placeholder(tf.float32, shape=(None, y_train.shape[1])),\n",
    "    'labels_mask': tf.placeholder(tf.int32),\n",
    "    'dropout': tf.placeholder_with_default(0., shape=()),\n",
    "    'num_features_nonzero': tf.placeholder(tf.int32)  # helper variable for sparse dropout\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = model_func(placeholders, input_dim=features[2][1], logging=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(features, support, labels, mask, placeholders):\n",
    "    t_test = time.time()\n",
    "    feed_dict_val = construct_feed_dict(features, support, labels, mask, placeholders)\n",
    "    outs_val = sess.run([model.loss, model.accuracy], feed_dict=feed_dict_val)\n",
    "    return outs_val[0], outs_val[1], (time.time() - t_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost_val = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 train_loss= 1.95399 train_acc= 0.07143 val_loss= 1.95070 val_acc= 0.20600 time= 0.05210\n",
      "Epoch: 0011 train_loss= 1.87589 train_acc= 0.72143 val_loss= 1.91122 val_acc= 0.52000 time= 0.02722\n",
      "Epoch: 0021 train_loss= 1.76760 train_acc= 0.77857 val_loss= 1.86662 val_acc= 0.59200 time= 0.03230\n",
      "Epoch: 0031 train_loss= 1.62944 train_acc= 0.80000 val_loss= 1.80741 val_acc= 0.64400 time= 0.02754\n",
      "Epoch: 0041 train_loss= 1.49882 train_acc= 0.85000 val_loss= 1.73166 val_acc= 0.69200 time= 0.03075\n",
      "Epoch: 0051 train_loss= 1.38851 train_acc= 0.85714 val_loss= 1.64305 val_acc= 0.73600 time= 0.03151\n",
      "Epoch: 0061 train_loss= 1.18911 train_acc= 0.89286 val_loss= 1.55086 val_acc= 0.75600 time= 0.02384\n",
      "Epoch: 0071 train_loss= 1.09616 train_acc= 0.88571 val_loss= 1.46357 val_acc= 0.77400 time= 0.02704\n",
      "Epoch: 0081 train_loss= 0.97463 train_acc= 0.91429 val_loss= 1.39013 val_acc= 0.77600 time= 0.02587\n",
      "Epoch: 0091 train_loss= 0.92480 train_acc= 0.95000 val_loss= 1.32596 val_acc= 0.78000 time= 0.02873\n",
      "Epoch: 0101 train_loss= 0.86087 train_acc= 0.95714 val_loss= 1.27814 val_acc= 0.78400 time= 0.02831\n",
      "Epoch: 0111 train_loss= 0.83708 train_acc= 0.95000 val_loss= 1.24008 val_acc= 0.77800 time= 0.02529\n",
      "Epoch: 0121 train_loss= 0.77942 train_acc= 0.95000 val_loss= 1.20875 val_acc= 0.78400 time= 0.02995\n",
      "Epoch: 0131 train_loss= 0.76718 train_acc= 0.94286 val_loss= 1.16783 val_acc= 0.78200 time= 0.02935\n",
      "Epoch: 0141 train_loss= 0.71716 train_acc= 0.94286 val_loss= 1.15020 val_acc= 0.78400 time= 0.02731\n",
      "Epoch: 0151 train_loss= 0.69488 train_acc= 0.97143 val_loss= 1.13300 val_acc= 0.78400 time= 0.02832\n",
      "Epoch: 0161 train_loss= 0.70023 train_acc= 0.97143 val_loss= 1.11047 val_acc= 0.78200 time= 0.03013\n",
      "Epoch: 0171 train_loss= 0.72109 train_acc= 0.92857 val_loss= 1.09840 val_acc= 0.78200 time= 0.02906\n",
      "Epoch: 0181 train_loss= 0.62900 train_acc= 0.97857 val_loss= 1.07482 val_acc= 0.78600 time= 0.02891\n",
      "Epoch: 0191 train_loss= 0.60898 train_acc= 0.98571 val_loss= 1.06063 val_acc= 0.78200 time= 0.02924\n",
      "Optimization Finished!\n",
      "Epoch: 0200 train_loss= 0.59147 train_acc= 0.97143 val_loss= 1.04964 val_acc= 0.77800 time= 0.02950\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(FLAGS.epochs):\n",
    "\n",
    "    t = time.time()\n",
    "    # Construct feed dictionary\n",
    "    feed_dict = construct_feed_dict(features, support, y_train, train_mask, placeholders)\n",
    "    feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n",
    "\n",
    "    # Training step\n",
    "    outs = sess.run([model.opt_op, model.loss, model.accuracy], feed_dict=feed_dict)\n",
    "\n",
    "    # Validation\n",
    "    cost, acc, duration = evaluate(features, support, y_val, val_mask, placeholders)\n",
    "    cost_val.append(cost)\n",
    "\n",
    "    # Print results\n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(outs[1]),\n",
    "              \"train_acc=\", \"{:.5f}\".format(outs[2]), \"val_loss=\", \"{:.5f}\".format(cost),\n",
    "              \"val_acc=\", \"{:.5f}\".format(acc), \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "\n",
    "    if epoch > FLAGS.early_stopping and cost_val[-1] > np.mean(cost_val[-(FLAGS.early_stopping+1):-1]):\n",
    "        print(\"Early stopping...\")\n",
    "        break\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "\n",
    "print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(outs[1]),\n",
    "              \"train_acc=\", \"{:.5f}\".format(outs[2]), \"val_loss=\", \"{:.5f}\".format(cost),\n",
    "              \"val_acc=\", \"{:.5f}\".format(acc), \"time=\", \"{:.5f}\".format(time.time() - t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: cost= 1.01263 accuracy= 0.81400 time= 0.01390\n"
     ]
    }
   ],
   "source": [
    "test_cost, test_acc, test_duration = evaluate(features, support, y_test, test_mask, placeholders)\n",
    "print(\"Test set results:\", \"cost=\", \"{:.5f}\".format(test_cost),\n",
    "      \"accuracy=\", \"{:.5f}\".format(test_acc), \"time=\", \"{:.5f}\".format(test_duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "   * I could use preprocessing as well: but I will have different features (not word occurances) - except if I make counters about the number of given degree neighbours ... _(That is cheating... for semi-classification)_\n",
    "   * If my features will be various centrality scores of the given node, than it should not be row normalized!!! rather\n",
    "   column normalized...\n",
    "   \n",
    "### Target\n",
    "   * binary (active-offline node)\n",
    "   * multiclass (the number of incoming mentions of a node - in logscale groups)\n",
    "   \n",
    "### TODO: What features should I generate?\n",
    "   * **simple:** node activity in train period (binary)\n",
    "   * **simple:** (the number of incoming mentions of a node - in logscale groups) in the train period\n",
    "   * **complex:** set various node centrality measure values (calculated for train period) as features for nodes\n",
    "   * **complex:** inter-event time statistics for nodes in the training period\n",
    "   * **hard:** textual context based features...\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:dm-3-env]",
   "language": "python",
   "name": "conda-env-dm-3-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
